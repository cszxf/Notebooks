# 基础

[计算机视觉基本任务综述](https://zhuanlan.zhihu.com/p/262697114)

![img](https://ah0aangfha.feishu.cn/space/api/box/stream/download/asynccode/?code=M2JmZGFmZWM1Njk3NmI4NGYxNjQwNDE0NWY5N2RiMzlfRENnMUlqbG1KUWZFNHY4dlNBZ2RBYkJJYlZYRVVMOTlfVG9rZW46Ym94Y25DV3UwUWhLbzQ1eXFiVHdwSE84VnpmXzE2NDYzODUzMDc6MTY0NjM4ODkwN19WNA)

![img](https://ah0aangfha.feishu.cn/space/api/box/stream/download/asynccode/?code=NDIxODE0NWRkYTZiODUzMTEyZDJkNTYwY2RkMmUzZDFfaExVc1N6R3hIZW5mR3JzNGdMNjR5TDFnNDE0VjM0QWhfVG9rZW46Ym94Y254Q0xJV0xLS3lwRG9Gb3luMER3RFhnXzE2NDYzODUzMDc6MTY0NjM4ODkwN19WNA)

 

N是输入图片长度，K是滤波器长度（卷积核大小），S是步长

 

## 干货集锦

[机器学习原理](https://www.cntofu.com/book/85/index.html)

[李宏毅机器学习笔记](https://datawhalechina.github.io/leeml-notes/#/)

[李宏毅Bilibili](https://www.bilibili.com/video/av59538266)

[白板推导](https://blog.csdn.net/qq_41485273/article/details/111563979)

 

## 信息论

![img](https://ah0aangfha.feishu.cn/space/api/box/stream/download/asynccode/?code=YzgwMzdkZmVkODg2MTI4NGM0N2QwMTA2MTE2OTE4OTFfaUhRVGg2MnEwcEllTldNZlBrZGhoZkF5a2NPUDRWWDRfVG9rZW46Ym94Y25vWlBmU3NTbXNRZDducDh1S1JpUVk4XzE2NDYzODUzMDc6MTY0NjM4ODkwN19WNA)

> I(X;Y) = I(Y;X) = H(Y)-H(Y/X) = H(X) – H(X/Y) 

> H(X) + H(Y) = H(X,Y) + I(X;Y)

> I(X;Y)=H(X) + H(Y) – H(X,Y)

# 基础概念

[详解残差网络](https://zhuanlan.zhihu.com/p/42706477)

[Singular Value Decomposition（SVD奇异值分解）](https://zhuanlan.zhihu.com/p/68386882)

[数据挖掘|概率图模型（一）](https://zhuanlan.zhihu.com/p/22751416)

[查准率，准确率，查全率](https://www.zhihu.com/question/19645541)

[F1值](https://zhuanlan.zhihu.com/p/97870600)

[自监督学习](https://zhuanlan.zhihu.com/p/66063089)

 [超详细的NLP预训练语言模型总结清单](https://mp.weixin.qq.com/s/b2q3nexPR7Amx5ficdBh-Q)

[TF-IDF算法介绍及实现](https://blog.csdn.net/asialee_bird/article/details/81486700)

[马尔可夫毯（Markov Blanket）](https://www.zhihu.com/question/20446337)

[端到端学习](https://www.cnblogs.com/wisir/p/12556353.html)

[特征工程与表示学习](https://zhuanlan.zhihu.com/p/41521695)

[图嵌入](https://zhuanlan.zhihu.com/p/62629465)

[图神经网络](https://zhuanlan.zhihu.com/p/75307407?from_voters_page=true)

[多模态特征表示和融合](https://blog.csdn.net/weixin_48629412/article/details/111174968)

[Bert详解](https://zhuanlan.zhihu.com/p/54356280)

维特比算法

> [维特比算法](https://blog.csdn.net/StarLib/article/details/106904606)实现

> [如何通俗地讲解 viterbi 算法？](https://www.zhihu.com/question/20136144)

### 

##  Normalization

[特征工程中的「归一化」有什么作用？](https://www.zhihu.com/question/20455227/answer/370658612)

[标准化和归一化，请勿混为一谈，透彻理解数据变换](https://blog.csdn.net/weixin_36604953/article/details/102652160)

[什么是批标准化 (Batch Normalization)](https://zhuanlan.zhihu.com/p/24810318)

[标准化和归一化什么区别？](https://www.zhihu.com/question/20467170)

1. > 缩放到均值为0，方差为1（**Standardization——**StandardScaler()）

2. > 缩放到0和1之间（**Standardization——**MinMaxScaler()）

3. > 缩放到-1和1之间（**Standardization——**MaxAbsScaler()）

4. > 缩放到0和1之间，保留原始数据的分布（**Normalization——**Normalizer()

[BatchNormalization、LayerNormalization、InstanceNorm、GroupNorm、SwitchableNorm总结](https://blog.csdn.net/liuxiao214/article/details/81037416)

# 经典算法

## 机器学习十大算法

[机器学习十大算法系列](https://blog.csdn.net/v_july_v/category_9261611.html)

### logistic回归

[logistic回归](https://www.cnblogs.com/geo-will/p/10468356.html)

### EM

> [EM算法](https://blog.csdn.net/u010834867/article/details/90762296)

> [如何通俗理解EM算法](https://blog.csdn.net/v_july_v/article/details/81708386)

> [从EM到Variational EM](https://zhuanlan.zhihu.com/p/32925505)

> [LDA-隐狄利克雷分布-主题模型](https://blog.csdn.net/u012771351/article/details/53032365)

### 贝叶斯

[机器学习|算法笔记-朴素贝叶斯（Naive Bayesian）](https://www.cnblogs.com/geo-will/p/10468401.html)

[从贝叶斯谈到贝叶斯网络](https://blog.csdn.net/v_july_v/article/details/40984699)

[朴素贝叶斯 VS 逻辑回归 区别](https://blog.csdn.net/cjneo/article/details/45167223)

### 支持向量机（SVM）

[推导 | SVM详解（1）SVM基本型](https://zhuanlan.zhihu.com/p/35755150)

###  CRF

> [CRF条件随机场](https://zhuanlan.zhihu.com/p/70067113)

> [马尔可夫随机场以及条件随机场](https://zhuanlan.zhihu.com/p/112980214)

> [最通俗易懂的BiLSTM-CRF模型中的CRF层讲](https://www.sohu.com/a/341284906_787107)

> [从隐马尔科夫到条件随机场](https://anxiang1836.github.io/2019/11/05/NLP_From_HMM_to_CRF/)

### HMM

> \* [HMM隐马尔科夫模型](https://zhuanlan.zhihu.com/p/29938926) 

> \* [HMM](https://www.cnblogs.com/pinard/p/6945257.html)

> \* [HMM2](https://blog.csdn.net/weixin_42175217/article/details/105442777?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_baidulandingword-0&spm=1001.2101.3001.4242)

> [隐马尔科夫模型](https://blog.csdn.net/hudashi/article/details/87867916)

隐马尔可夫模型是 先生成状态序列，然后由状态序列生成观测序列，即是先 P(Z) , 再 P(O|Z), 所以拟合的是 P(O, Z)也就是联合概率分布而判别模型拟合的是条件概率分布。HMM在数据量较少的时候，会脑补数据，性能更好。

### 决策树

[决策树(Decision Tree)：通俗易懂之介绍](https://zhuanlan.zhihu.com/p/30059442)

[决策树--信息增益，信息增益比，Geni指数的理解](https://www.cnblogs.com/muzixi/p/6566803.html)



## 其他算法

[随机森林算法及其实现（Random Forest）](https://blog.csdn.net/yangyin007/article/details/82385967)

# 



 

####  

# NER

## 数据集

[命名实体识别学习-数据集介绍-conll03](https://www.datafountain.cn/datasets/5684)

[LSTM+CRF序列标注](https://blog.csdn.net/StarLib/article/details/106933974)

# 关系抽取

[简介](https://www.cnblogs.com/sandwichnlp/p/12049829.html)

[实体关系联合抽取总结](https://zhuanlan.zhihu.com/p/74886839)

[结构学习：序列标注](https://blog.csdn.net/dugudaibo/article/details/79120627)

 

# attention

[MultiHeadAttention实现详解](https://zhuanlan.zhihu.com/p/358206572)

[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

[循环神经网络（RNN）浅析](https://www.jianshu.com/p/87aa03352eb9)

 [序列标注](https://www.jianshu.com/p/5a5bcfe5c185)

[序列标注和中文命名实体识别](https://www.jianshu.com/p/c7c3ace12044)

 [Seq2Seq模型概述](https://www.jianshu.com/p/b2b95f945a98)

[人人都能看懂的LSTM](https://zhuanlan.zhihu.com/p/32085405)

 [attention机制原理及简单实现](https://www.jianshu.com/p/1d67638139da)

[深入理解shortcut](https://blog.csdn.net/baidu_29571167/article/details/89012223)

[Encoder-Decoder](https://blog.csdn.net/qq_38906523/article/details/79838000)



[词向量之BERT](https://zhuanlan.zhihu.com/p/48612853)

# Transformer

[bert的输入输出是什么](https://zhuanlan.zhihu.com/p/248017234)

[Transformer模型详解（图解最完整版）](https://zhuanlan.zhihu.com/p/338817680)

[深度学习----Transformer模型之图示进阶篇](https://blog.csdn.net/Sakura55/article/details/86679826)

 [李宏毅-Transformer](https://www.bilibili.com/video/BV1J441137V6)

 [60分钟带你掌握NLP BERT理论与实战](https://www.bilibili.com/video/BV1H441187js?p=1)

 [李宏毅-ELMO, BERT, GPT讲解](https://www.bilibili.com/video/BV17441137fa/?spm_id_from=333.788.videocard.11)

[nlp中的Attention注意力机制+Transformer详解](https://zhuanlan.zhihu.com/p/53682800)

[NLP中的RNN、Seq2Seq与attention注意力机制](https://zhuanlan.zhihu.com/p/52119092)

[nlp中的Attention注意力机制+Transformer详解](https://zhuanlan.zhihu.com/p/53682800)

[详解Transformer （Attention Is All You Need）](https://zhuanlan.zhihu.com/p/48508221)

# 文本分类

 [文本分类实战](https://www.cnblogs.com/jiangxinyang/p/10207273.html)



# 经典模型

## VGG16

[VGG16详解](https://www.cnblogs.com/lfri/p/10493408.html)

## ResNet





# 多模态

[跨模态检索](https://blog.csdn.net/qq_39388410/article/details/105907097)

[多模态融合](https://blog.csdn.net/qq_39388410/article/details/105145074)

# 问答

[生成模型和判别模型的区别](https://www.zhihu.com/question/20446337)



# TEMP



# Others

[自监督、半监督、无监督学习，傻傻分不清楚？最新综述来帮你！](https://blog.csdn.net/moxibingdao/article/details/106667760)

[有监督学习和无监督学习举例_对比自监督学习](https://blog.csdn.net/weixin_39612677/article/details/110394322)

![img](https://ah0aangfha.feishu.cn/space/api/box/stream/download/asynccode/?code=N2E1ZmU3MTE5MzVmYTNjOTBhNjU5YWFjZjJjNjBkNGVfYU95eEtCcEtNVVhibExKR3ltQUg0ZlZqbTYzakVRM2NfVG9rZW46Ym94Y25jQ3dCMkdocVZsWjFuSjZCcHJyQmJlXzE2NDYzODUzMDc6MTY0NjM4ODkwN19WNA)

 

 

 

建议路线，ng课程入门，知道有哪些算法，大致怎么做，然后去kaggle打个入门赛，别做特征工程，把会的算法全用上。然后放下比赛，开始读这本书，同时看机器学习基石或其他比较数学化的进阶课程，这一步不需要你敲代码，你要会的是滚瓜烂熟的推导，做到这一步，再去kaggle参加奖金赛，阅读kernel，学习state of the art 模型，学习特征工程，再在学习过程中阅读最新的论文或者经典的论文，不断迭代这个过程，别淹死在什么机器学习实战上，有现成的轮子不用，非得费那个劲，除非你科班毕业，代码能力扎实，不然你能不能从头实现一遍决策树对你找不找到工作没有任何一毛钱关系。笔试不会考你如何实现hmm，只会考数据结构与算法，面试只会让你推导。